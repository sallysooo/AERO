{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665d0a4b-b58f-4f31-8a67-8b92fda0d372",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scapy==2.4.4\n",
    "\n",
    "from IPython.display import display\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scapy.utils import RawPcapReader\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import skew\n",
    "\n",
    "# 1. TimeSeriesGenerator\n",
    "class TimeseriesGenerator:\n",
    "    def __init__(self, data, length, sampling_rate=1, stride=1,\n",
    "                 start_index=0, end_index=None,\n",
    "                 shuffle=False, reverse=False, batch_size=128, label=None):\n",
    "        self.data = data\n",
    "        self.length = length\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.stride = stride\n",
    "        self.start_index = start_index + length\n",
    "        if end_index is None:\n",
    "            end_index = len(data)\n",
    "        self.end_index = end_index\n",
    "        self.shuffle = shuffle\n",
    "        self.reverse = reverse\n",
    "        self.batch_size = batch_size\n",
    "        self.label = label if label is None else np.array(label)\n",
    "        if self.start_index > self.end_index:\n",
    "            raise ValueError(\n",
    "                \"`start_index+length=%i > end_index=%i` \"\n",
    "                \"is disallowed, as no part of the sequence \"\n",
    "                \"would be left to be used as current step.\"\n",
    "                % (self.start_index, self.end_index)\n",
    "            )\n",
    "\n",
    "    def __len__(self):\n",
    "        return (self.end_index - self.start_index + self.batch_size * self.stride) // (self.batch_size * self.stride)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        rows = self.__index_to_row__(index)\n",
    "        samples, y = self.__compile_batch__(rows)\n",
    "        return samples, y\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i in range(len(self)):\n",
    "            yield self[i]\n",
    "    \n",
    "    def __index_to_row__(self, index):  # Returns a list of rows that will compose a given batch (index). len(rows) is equal to the batch size.\n",
    "        if self.shuffle:\n",
    "            rows = np.random.randint(self.start_index, self.end_index + 1, size=self.batch_size)\n",
    "        else:\n",
    "            i = self.start_index + self.batch_size * self.stride * index\n",
    "            rows = np.arange(i, min(i + self.batch_size * self.stride, self.end_index + 1), self.stride)\n",
    "        return rows\n",
    "\n",
    "    def __compile_batch__(self, rows):  # Generate time series features for each given row.\n",
    "        samples = np.array([self.data[row - self.length: row: self.sampling_rate] for row in rows])\n",
    "        if self.reverse:\n",
    "            samples = samples[:, ::-1, ...]\n",
    "        if self.length == 1:\n",
    "            samples = np.squeeze(samples)\n",
    "\n",
    "        if self.label is None:\n",
    "            return samples, samples\n",
    "        else:\n",
    "            return samples, self.label[rows - self.length]\n",
    "\n",
    "    @property\n",
    "    def output_shape(self):\n",
    "        x, y = self[0]\n",
    "        return x.shape, y.shape\n",
    "\n",
    "    @property\n",
    "    def num_samples(self):\n",
    "        count = 0\n",
    "        for x, y in self:\n",
    "            count += x.shape[0]\n",
    "        return count\n",
    "\n",
    "    def __str__(self):\n",
    "        return '<TimeseriesGenerator data.shape={} / num_batches={:,} / output_shape={}>'.format(\n",
    "            self.data.shape, len(self), self.output_shape,\n",
    "        )\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "# 2. Load Dataset\n",
    "class Dataset:\n",
    "    def __init__(self, df: pd.DataFrame, trim_etc_protocols=True):\n",
    "        if trim_etc_protocols:\n",
    "            self.df = df[df['ProtocolType'] != ''].copy()\n",
    "        else:\n",
    "            self.df = df\n",
    "        assert self.df['abstime'].is_monotonic_increasing\n",
    "        assert self.df['monotime'].is_monotonic_increasing\n",
    "\n",
    "    @classmethod\n",
    "    def _load_towids_dataset(cls, path_pcap, usec_unit, path_csv=None, **kwargs):\n",
    "        # assert scapy.__version__ == '2.4.4', 'scapy version mismatch.'\n",
    "\n",
    "        reader = RawPcapReader(str(path_pcap))\n",
    "        list_output = list()\n",
    "        for idx, (payload, metadata) in tqdm(enumerate(reader), desc='Parsing the pcap file...'):\n",
    "            sec, usec, wirelen, caplen = metadata\n",
    "            list_output.append((sec, usec, wirelen, caplen, payload))\n",
    "        df_pcap = pd.DataFrame(list_output, columns=['sec', 'usec', 'wirelen', 'caplen', 'payload'])\n",
    "\n",
    "        if path_csv:\n",
    "            df_label = pd.read_csv(path_csv, header=None, names=['idx', 'label', 'y_desc'])\n",
    "            assert df_pcap.shape[0] == df_label.shape[0], \\\n",
    "                f'Record count mismatch. {df_pcap.shape=}, {df_label.shape=}'\n",
    "            assert (df_label['idx'].diff().bfill() == 1).all(), 'Field `idx` does not increase sequentially.'\n",
    "            df_label['y'] = df_label['label'].map({'Normal': 0, 'Abnormal': 1})\n",
    "        else:\n",
    "            df_label = pd.DataFrame(index=df_pcap.index)\n",
    "            df_label['y'] = 0\n",
    "            df_label['y_desc'] = 'Normal'\n",
    "        abstime = pd.to_datetime(df_pcap['sec'], unit='s') + pd.to_timedelta(df_pcap['usec'], unit=usec_unit)\n",
    "        dupcounts = abstime.duplicated(keep=False).sum()\n",
    "\n",
    "        if dupcounts > 0:\n",
    "            print(f'There were {dupcounts} distinct timestamps.', end=' ')\n",
    "            for _ in range(100):\n",
    "                duplicated = abstime.duplicated()\n",
    "                if duplicated.sum() == 0:\n",
    "                    break\n",
    "                abstime[duplicated] += pd.Timedelta(milliseconds=1)\n",
    "            else:\n",
    "                raise ValueError('Something went wrong.')\n",
    "            print(f'-> {_} correction(s).')\n",
    "\n",
    "        monotime = (abstime - abstime.min()).dt.total_seconds()\n",
    "        df_pcap['payload'] = df_pcap['payload'].map(lambda x: np.frombuffer(x, dtype='uint8'))\n",
    "\n",
    "        df: pd.DataFrame = pd.concat([\n",
    "            abstime.rename('abstime'),\n",
    "            monotime.rename('monotime'),\n",
    "            df_pcap[['wirelen', 'caplen', 'payload']],\n",
    "            df_label[['y', 'y_desc']]\n",
    "        ], axis=1)\n",
    "\n",
    "        df = df.sort_values('abstime')\n",
    "        assert df['abstime'].is_monotonic_increasing\n",
    "        assert df['monotime'].is_monotonic_increasing\n",
    "\n",
    "        # Protocol specification\n",
    "        df['ProtocolType'] = ''\n",
    "        df.loc[df['wirelen'] == 60, 'ProtocolType'] = 'UDP'\n",
    "        df.loc[df['wirelen'].isin([68, 90]), 'ProtocolType'] = 'PTP'\n",
    "        df.loc[df['wirelen'].isin([82, 434]), 'ProtocolType'] = 'AVTP'\n",
    "        # special treatment\n",
    "        df.loc[df['y_desc'] == 'P_I', 'ProtocolType'] = 'PTP'\n",
    "\n",
    "        return cls(df, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def towids_train(cls, **kwargs):\n",
    "        return cls._load_towids_dataset(\n",
    "            Path('dataset/Automotive_Ethernet_with_Attack_original_10_17_19_50_training.pcap'),\n",
    "            'ns',\n",
    "            Path('dataset/y_train.csv'),\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def towids_test(cls, **kwargs):\n",
    "        return cls._load_towids_dataset(\n",
    "            Path('dataset/Automotive_Ethernet_with_Attack_original_10_17_20_04_test.pcap'),\n",
    "            'ns',\n",
    "            Path('dataset/y_test.csv'),\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def do_label(self, window_size) -> np.ndarray:\n",
    "        y = self.df.rolling(window=window_size)['y'].max().dropna().astype('int32').values\n",
    "        assert isinstance(y, np.ndarray)\n",
    "        return y\n",
    "\n",
    "    def trim(self, time_start=None, time_end=None, is_absolute=None):\n",
    "        assert is_absolute is not None\n",
    "        monotime_min = self.df['monotime'].min()\n",
    "        monotime_max = self.df['monotime'].max()\n",
    "\n",
    "        if time_start is not None:\n",
    "            if is_absolute is False:\n",
    "                time_start = monotime_min + time_start\n",
    "            assert monotime_min < time_start\n",
    "        else:\n",
    "            time_start = monotime_min\n",
    "\n",
    "        if time_end is not None:\n",
    "            if is_absolute is False:\n",
    "                time_end = monotime_max - time_end\n",
    "            assert time_end < monotime_max\n",
    "        else:\n",
    "            time_end = monotime_max\n",
    "\n",
    "        df = self.df.query(f'{time_start} <= monotime <= {time_end}').copy()\n",
    "        # print('Before [{} ~ {}] / Required [{} ~ {}] / After [{} ~ {}]'.format(\n",
    "        #     monotime_min, monotime_max,\n",
    "        #     time_start, time_end,\n",
    "        #     df['monotime'].min(), df['monotime'].max()\n",
    "        # ))\n",
    "        return Dataset(df)\n",
    "        \n",
    "    # 2-1. Feature generator 1 (FG1)\n",
    "    def do_fg1_transition_matrix(self, window_size=2048) -> np.array:\n",
    "        # When the number of collected packets is n, a numpy array of shape = (n, 3, 3) should be the output\n",
    "        df = self.df\n",
    "        # proto_types = sorted(df['ProtocolType'].unique()) # ex) ['AVTP', 'PTP', 'UDP']\n",
    "        idx = {'AVTP': 0, 'PTP': 1, 'UDP': 2} # ex) {'AVTP': 0, 'PTP': 1, 'UDP': 2}\n",
    "        N = len(idx) # 3\n",
    "\n",
    "        # 1. ProtocolType sequence -> integer index\n",
    "        proto_seq = df['ProtocolType'].map(idx).values # [2, 0, 0, 1, 2]\n",
    "\n",
    "        # 2. generate T\n",
    "        def seq_to_transition_matrix(seq):\n",
    "          T = np.zeros((N, N), dtype=np.float32)\n",
    "          for i in range(len(seq) - 1):\n",
    "            a, b = seq[i], seq[i+1]\n",
    "            T[a, b] += 1\n",
    "          T /= (len(seq)-1) # normalization\n",
    "          return T\n",
    "\n",
    "        if len(proto_seq) < window_size:\n",
    "          raise ValueError(f\"Insufficient data length ({len(proto_seq)}) for window_size {window_size}\")\n",
    "\n",
    "        # checkpoint\n",
    "        print(\"Data shape:\", proto_seq.shape)\n",
    "        print(\"Window size:\", window_size)\n",
    "\n",
    "        # 3. sliding window using TimeseriesGenerator\n",
    "        generator = TimeseriesGenerator(proto_seq, length=window_size, sampling_rate=1, stride=1, batch_size=1, shuffle=False)\n",
    "\n",
    "        print(\"Generator length:\", len(generator))\n",
    "        # if len(generator) == 0:\n",
    "        #   print(\"Warning: Generator is empty! Check window_size and data length.\")\n",
    "        #   return np.zeros((0, N, N))\n",
    "\n",
    "        result = []\n",
    "        for X, _ in generator:\n",
    "          seq = X[0] # (window_size, )\n",
    "          T = seq_to_transition_matrix(seq)\n",
    "          result.append(T)\n",
    "\n",
    "        return np.stack(result) # (num_windows, N, N)\n",
    "\n",
    "\n",
    "    # 2-2. Feature generator 2 (FG2)\n",
    "    def do_fg2_payload(self, window_size=2048, byte_start=0x22, byte_end=0x22 + 9) -> np.array:\n",
    "        '''\n",
    "        - The paper's strategy is to take 9 bytes from the 0x22th byte for the payload loaded in each packet.  \n",
    "        - Short payloads should be padded with 0x00.\n",
    "        - When the number of collected packets is n, a numpy array with shape = (n, 9) should be generated. \n",
    "        - FG2 does not need to apply TimeseriesGenerator.\n",
    "        '''\n",
    "        assert byte_start < byte_end\n",
    "        num_bytes = byte_end - byte_start # 9\n",
    "\n",
    "        payloads = []\n",
    "        for arr in self.df['payload'].values:\n",
    "          segment = np.zeros(num_bytes, dtype=np.uint8) # [0, 0, 0, ..., 0]\n",
    "          arr_len = len(arr)\n",
    "          for i in range(num_bytes): # 9\n",
    "            if byte_start + i < arr_len:\n",
    "              segment[i] = arr[byte_start + i]\n",
    "          payloads.append(segment / 255.0)\n",
    "\n",
    "        return np.array(payloads) # (n ,9)\n",
    "\n",
    "\n",
    "    # 2-3. Feature generator 3 (FG3)\n",
    "    def do_fg3_statistics(self, window_size=2048, methods=('mean', 'std', 'skew')) -> np.array:\n",
    "        '''\n",
    "        - When the number of collected packets is n, a numpy array of shape=(n, 3, 3) should be generated.\n",
    "        - The <feature normalization strategy> described at the bottom right of page 5 of the paper must be implemented.\n",
    "        '''\n",
    "        df = self.df\n",
    "        # proto_types = sorted(df['ProtocolType'].unique()) # ex) ['AVTP', 'PTP', 'UDP']\n",
    "        idx = {'AVTP': 0, 'PTP': 1, 'UDP': 2} # ex) {'AVTP': 0, 'PTP': 1, 'UDP': 2}\n",
    "        N = len(idx) # ex) 3\n",
    "\n",
    "        monotime = df['monotime'].values\n",
    "        protos = df['ProtocolType'].map(idx).values\n",
    "\n",
    "        # each window is constructed as [window_size * 2]\n",
    "        generator = TimeseriesGenerator(\n",
    "            np.stack([monotime, protos], axis=1), # (n, 2)\n",
    "            length = window_size,\n",
    "            sampling_rate = 1,\n",
    "            stride = 1,\n",
    "            batch_size = 1,\n",
    "            shuffle = False\n",
    "            )\n",
    "\n",
    "        # checkpoint\n",
    "        print(\"Data shape:\", np.stack([monotime, protos], axis=1).shape)\n",
    "        print(\"Window size:\", window_size)\n",
    "\n",
    "        result = []\n",
    "        for X, _ in generator:\n",
    "          x_window = X[0] # (window_size, 2)\n",
    "          t = x_window[:, 0] # first column of 'monotime' [1.0, 1.2, 1.3, 2.0, ...]\n",
    "          p = x_window[:, 1].astype(int) # second column of 'protos(protocol index)' [0, 0, 1, 0]\n",
    "\n",
    "          stat_matrix = np.full((N, 3), 1e+7, dtype=np.float32) # Initialize default value to 1e+7\n",
    "\n",
    "          for i in range(N):\n",
    "            t_i = t[p == i] # time sequence of the ith protocol / t : [1.0, 1.2, 1.3, 2.0, ...] / p==i : [True, True, False, True, ...] / t[p==i] : [1.0, 1.2, 2.0] => select protocol by this workflow\n",
    "            if len(t_i) >= 2:\n",
    "                diffs = np.diff(t_i)\n",
    "                mean_val = np.mean(diffs)\n",
    "                stat_matrix[i, 0] = mean_val\n",
    "\n",
    "                if len(diffs) >= 2:\n",
    "                    std_val = np.std(diffs)\n",
    "                    stat_matrix[i, 1] = std_val\n",
    "                if len(diffs) >= 3:\n",
    "                    skew_val = np.abs(skew(diffs))\n",
    "                    stat_matrix[i, 2] = skew_val\n",
    "            \n",
    "          stat_matrix = np.where(stat_matrix == 0, 1e-7, stat_matrix)\n",
    "          stat_matrix = np.log10(stat_matrix)\n",
    "\n",
    "          result.append(stat_matrix)\n",
    "\n",
    "        return np.stack(result) # (num_windows, N, 3)\n",
    "\n",
    "dataset_train = Dataset.towids_train()\n",
    "dataset_test = Dataset.towids_test()\n",
    "\n",
    "\n",
    "# 3. Create train/validation/test sets by dividing the two packet dump datasets (dataset_train, dataset_test) into different time ranges.\n",
    "# Organize the number of malicious traffic (intrusion) and normal traffic (benign) in these sets into a table as below.\n",
    "# Arguments to be passed to the do function: [dataset, purpose, start time, end time, whether to remove the last 5 seconds of noise]\n",
    "\n",
    "args = [\n",
    "    [dataset_train, 'Train', 5, 60, False],\n",
    "    [dataset_train, 'Validation', 60, 71.11, False],\n",
    "    [dataset_train, 'Test', 71.11, None, True],\n",
    "    [dataset_test, 'Train', 5, 80, False],\n",
    "    [dataset_test, 'Validation', 80, 91.88, False],\n",
    "    [dataset_test, 'Test', 91.89, None, True],\n",
    "]\n",
    "\n",
    "def do(dataset, purpose, time_start, time_end, trim_last_5sec):\n",
    "    name = 'Packet dump 1' if dataset is dataset_train else 'Packet dump 2'\n",
    "\n",
    "    dataset = dataset.trim(time_start, time_end, is_absolute=True) # slice [time_start, time_end] part only in the entire dataset\n",
    "    if trim_last_5sec: # Remove noise remaining after the data collection step\n",
    "        dataset = dataset.trim(time_end=5, is_absolute=False)\n",
    "        time_end = dataset.df['monotime'].max() # Since 'time_end' has changed after removing noise, update it again with the actual maximum time\n",
    "    a = dataset.df['y'].value_counts() \n",
    "    a.name = name \n",
    "    a['Purpose'] = purpose \n",
    "    a['Time range'] = '[{:.2f}, {:.2f}]'.format(time_start, time_end)\n",
    "    a = a.rename({0: 'Benign', 1: 'Intrusion'}) # 0 as benign, 1 as intrusion\n",
    "    a = a.reindex(['Purpose', 'Time range', 'Benign', 'Intrusion'], fill_value=0)\n",
    "    return a, dataset\n",
    "\n",
    "\n",
    "list_output = list()\n",
    "list_dataset_sub = list() ######### From here, you can retrieve the Dataset instance as needed.\n",
    "for arg in args:\n",
    "    output, dataset_sub = do(*arg)\n",
    "    list_output.append(output)\n",
    "    list_dataset_sub.append(dataset_sub)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
