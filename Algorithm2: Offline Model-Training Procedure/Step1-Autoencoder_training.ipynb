{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "606bb0d0-b22f-4e09-a9d3-c7b0f8b156e9",
   "metadata": {},
   "source": [
    "## Loading Dataset as an input x — Defining Dataset Class\n",
    "**The paths to the files** are as follows:\n",
    "- 'dataset/y_test.csv'\n",
    "- 'dataset/y_train.csv'\n",
    "- 'dataset/Automotive_Ethernet_with_Attack_original_10_17_20_04_test.pcap'\n",
    "- 'dataset/Automotive_Ethernet_with_Attack_original_10_17_19_50_training.pcap'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95929b71-f573-4658-b3ff-16fa04ab59d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scapy==2.4.4 in /home/sallysooo/miniforge3/lib/python3.11/site-packages (2.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install scapy==2.4.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b579345-b2a4-4e91-b71f-9b8921cb1f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing the pcap file...: 1203737it [00:00, 1723370.84it/s]\n",
      "Parsing the pcap file...: 791611it [00:00, 1852064.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 2 distinct timestamps. -> 1 correction(s).\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scapy.utils import RawPcapReader\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import skew\n",
    "\n",
    "class TimeseriesGenerator:\n",
    "    pass # Implemented below\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, df: pd.DataFrame, trim_etc_protocols=True):\n",
    "        if trim_etc_protocols:\n",
    "            self.df = df[df['ProtocolType'] != ''].copy()\n",
    "        else:\n",
    "            self.df = df\n",
    "        assert self.df['abstime'].is_monotonic_increasing\n",
    "        assert self.df['monotime'].is_monotonic_increasing\n",
    "\n",
    "    @classmethod\n",
    "    def _load_towids_dataset(cls, path_pcap, usec_unit, path_csv=None, **kwargs):\n",
    "        # assert scapy.__version__ == '2.4.4', 'scapy version mismatch.'\n",
    "\n",
    "        reader = RawPcapReader(str(path_pcap))\n",
    "        list_output = list()\n",
    "        for idx, (payload, metadata) in tqdm(enumerate(reader), desc='Parsing the pcap file...'):\n",
    "            sec, usec, wirelen, caplen = metadata\n",
    "            list_output.append((sec, usec, wirelen, caplen, payload))\n",
    "        df_pcap = pd.DataFrame(list_output, columns=['sec', 'usec', 'wirelen', 'caplen', 'payload'])\n",
    "\n",
    "        if path_csv:\n",
    "            df_label = pd.read_csv(path_csv, header=None, names=['idx', 'label', 'y_desc'])\n",
    "            assert df_pcap.shape[0] == df_label.shape[0], \\\n",
    "                f'Record count mismatch. {df_pcap.shape=}, {df_label.shape=}'\n",
    "            assert (df_label['idx'].diff().bfill() == 1).all(), 'Field `idx` does not increase sequentially.'\n",
    "            df_label['y'] = df_label['label'].map({'Normal': 0, 'Abnormal': 1})\n",
    "        else:\n",
    "            df_label = pd.DataFrame(index=df_pcap.index)\n",
    "            df_label['y'] = 0\n",
    "            df_label['y_desc'] = 'Normal'\n",
    "        abstime = pd.to_datetime(df_pcap['sec'], unit='s') + pd.to_timedelta(df_pcap['usec'], unit=usec_unit)\n",
    "        dupcounts = abstime.duplicated(keep=False).sum()\n",
    "\n",
    "        if dupcounts > 0:\n",
    "            print(f'There were {dupcounts} distinct timestamps.', end=' ')\n",
    "            for _ in range(100):\n",
    "                duplicated = abstime.duplicated()\n",
    "                if duplicated.sum() == 0:\n",
    "                    break\n",
    "                abstime[duplicated] += pd.Timedelta(milliseconds=1)\n",
    "            else:\n",
    "                raise ValueError('Something went wrong.')\n",
    "            print(f'-> {_} correction(s).')\n",
    "\n",
    "        monotime = (abstime - abstime.min()).dt.total_seconds()\n",
    "        df_pcap['payload'] = df_pcap['payload'].map(lambda x: np.frombuffer(x, dtype='uint8'))\n",
    "\n",
    "        df: pd.DataFrame = pd.concat([\n",
    "            abstime.rename('abstime'),\n",
    "            monotime.rename('monotime'),\n",
    "            df_pcap[['wirelen', 'caplen', 'payload']],\n",
    "            df_label[['y', 'y_desc']]\n",
    "        ], axis=1)\n",
    "\n",
    "        df = df.sort_values('abstime')\n",
    "        assert df['abstime'].is_monotonic_increasing\n",
    "        assert df['monotime'].is_monotonic_increasing\n",
    "\n",
    "        # Protocol specification\n",
    "        df['ProtocolType'] = ''\n",
    "        df.loc[df['wirelen'] == 60, 'ProtocolType'] = 'UDP'\n",
    "        df.loc[df['wirelen'].isin([68, 90]), 'ProtocolType'] = 'PTP'\n",
    "        df.loc[df['wirelen'].isin([82, 434]), 'ProtocolType'] = 'AVTP'\n",
    "        # special treatment\n",
    "        df.loc[df['y_desc'] == 'P_I', 'ProtocolType'] = 'PTP'\n",
    "\n",
    "        return cls(df, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def towids_train(cls, **kwargs):\n",
    "        return cls._load_towids_dataset(\n",
    "            Path('dataset/Automotive_Ethernet_with_Attack_original_10_17_19_50_training.pcap'),\n",
    "            'ns',\n",
    "            Path('dataset/y_train.csv'),\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def towids_test(cls, **kwargs):\n",
    "        return cls._load_towids_dataset(\n",
    "            Path('dataset/Automotive_Ethernet_with_Attack_original_10_17_20_04_test.pcap'),\n",
    "            'ns',\n",
    "            Path('dataset/y_test.csv'),\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def do_label(self, window_size) -> np.ndarray:\n",
    "        y = self.df.rolling(window=window_size)['y'].max().dropna().astype('int32').values\n",
    "        assert isinstance(y, np.ndarray)\n",
    "        return y\n",
    "\n",
    "    def trim(self, time_start=None, time_end=None, is_absolute=None):\n",
    "        assert is_absolute is not None\n",
    "        monotime_min = self.df['monotime'].min()\n",
    "        monotime_max = self.df['monotime'].max()\n",
    "\n",
    "        if time_start is not None:\n",
    "            if is_absolute is False:\n",
    "                time_start = monotime_min + time_start\n",
    "            assert monotime_min < time_start\n",
    "        else:\n",
    "            time_start = monotime_min\n",
    "\n",
    "        if time_end is not None:\n",
    "            if is_absolute is False:\n",
    "                time_end = monotime_max - time_end\n",
    "            assert time_end < monotime_max\n",
    "        else:\n",
    "            time_end = monotime_max\n",
    "\n",
    "        df = self.df.query(f'{time_start} <= monotime <= {time_end}').copy()\n",
    "        # print('Before [{} ~ {}] / Required [{} ~ {}] / After [{} ~ {}]'.format(\n",
    "        #     monotime_min, monotime_max,\n",
    "        #     time_start, time_end,\n",
    "        #     df['monotime'].min(), df['monotime'].max()\n",
    "        # ))\n",
    "        return Dataset(df)\n",
    "        \n",
    "    # Feature generator 1 (FG1)\n",
    "    def do_fg1_transition_matrix(self, window_size=2048) -> np.array:\n",
    "        # When the number of collected packets is n, a numpy array of shape = (n, 3, 3) should be the output\n",
    "        df = self.df\n",
    "        # proto_types = sorted(df['ProtocolType'].unique()) # ex) ['AVTP', 'PTP', 'UDP']\n",
    "        idx = {'AVTP': 0, 'PTP': 1, 'UDP': 2} # ex) {'AVTP': 0, 'PTP': 1, 'UDP': 2}\n",
    "        N = len(idx) # 3\n",
    "\n",
    "        # 1. ProtocolType sequence -> integer index\n",
    "        proto_seq = df['ProtocolType'].map(idx).values # [2, 0, 0, 1, 2]\n",
    "\n",
    "        # 2. generate T\n",
    "        def seq_to_transition_matrix(seq):\n",
    "          T = np.zeros((N, N), dtype=np.float32)\n",
    "          for i in range(len(seq) - 1):\n",
    "            a, b = seq[i], seq[i+1]\n",
    "            T[a, b] += 1\n",
    "          T /= (len(seq)-1) # normalization\n",
    "          return T\n",
    "\n",
    "        if len(proto_seq) < window_size:\n",
    "          raise ValueError(f\"Insufficient data length ({len(proto_seq)}) for window_size {window_size}\")\n",
    "\n",
    "        # checkpoint\n",
    "        print(\"Data shape:\", proto_seq.shape)\n",
    "        print(\"Window size:\", window_size)\n",
    "\n",
    "        # 3. sliding window using TimeseriesGenerator\n",
    "        generator = TimeseriesGenerator(proto_seq, length=window_size, sampling_rate=1, stride=1, batch_size=1, shuffle=False)\n",
    "\n",
    "        print(\"Generator length:\", len(generator))\n",
    "        # if len(generator) == 0:\n",
    "        #   print(\"Warning: Generator is empty! Check window_size and data length.\")\n",
    "        #   return np.zeros((0, N, N))\n",
    "\n",
    "        result = []\n",
    "        for X, _ in generator:\n",
    "          seq = X[0] # (window_size, )\n",
    "          T = seq_to_transition_matrix(seq)\n",
    "          result.append(T)\n",
    "\n",
    "        return np.stack(result) # (num_windows, N, N)\n",
    "\n",
    "\n",
    "    # Feature generator 2 (FG2)\n",
    "    def do_fg2_payload(self, window_size=2048, byte_start=0x22, byte_end=0x22 + 9) -> np.array:\n",
    "        '''\n",
    "        - The paper's strategy is to take 9 bytes from the 0x22th byte for the payload loaded in each packet.  \n",
    "        - Short payloads should be padded with 0x00.\n",
    "        - When the number of collected packets is n, a numpy array with shape = (n, 9) should be generated. \n",
    "        - FG2 does not need to apply TimeseriesGenerator.\n",
    "        '''\n",
    "        assert byte_start < byte_end\n",
    "        num_bytes = byte_end - byte_start # 9\n",
    "\n",
    "        payloads = []\n",
    "        for arr in self.df['payload'].values:\n",
    "          segment = np.zeros(num_bytes, dtype=np.uint8) # [0, 0, 0, ..., 0]\n",
    "          arr_len = len(arr)\n",
    "          for i in range(num_bytes): # 9\n",
    "            if byte_start + i < arr_len:\n",
    "              segment[i] = arr[byte_start + i]\n",
    "          payloads.append(segment / 255.0)\n",
    "\n",
    "        return np.array(payloads) # (n ,9)\n",
    "\n",
    "\n",
    "    # Feature generator 3 (FG3)\n",
    "    def do_fg3_statistics(self, window_size=2048, methods=('mean', 'std', 'skew')) -> np.array:\n",
    "        '''\n",
    "        - When the number of collected packets is n, a numpy array of shape=(n, 3, 3) should be generated.\n",
    "        - The <feature normalization strategy> described at the bottom right of page 5 of the paper must be implemented.\n",
    "        '''\n",
    "        df = self.df\n",
    "        # proto_types = sorted(df['ProtocolType'].unique()) # ex) ['AVTP', 'PTP', 'UDP']\n",
    "        idx = {'AVTP': 0, 'PTP': 1, 'UDP': 2} # ex) {'AVTP': 0, 'PTP': 1, 'UDP': 2}\n",
    "        N = len(idx) # ex) 3\n",
    "\n",
    "        monotime = df['monotime'].values\n",
    "        protos = df['ProtocolType'].map(idx).values\n",
    "\n",
    "        # each window is constructed as [window_size * 2]\n",
    "        generator = TimeseriesGenerator(\n",
    "            np.stack([monotime, protos], axis=1), # (n, 2)\n",
    "            length = window_size,\n",
    "            sampling_rate = 1,\n",
    "            stride = 1,\n",
    "            batch_size = 1,\n",
    "            shuffle = False\n",
    "            )\n",
    "\n",
    "        # checkpoint\n",
    "        print(\"Data shape:\", np.stack([monotime, protos], axis=1).shape)\n",
    "        print(\"Window size:\", window_size)\n",
    "\n",
    "        result = []\n",
    "        for X, _ in generator:\n",
    "          x_window = X[0] # (window_size, 2)\n",
    "          t = x_window[:, 0] # first column of 'monotime' [1.0, 1.2, 1.3, 2.0, ...]\n",
    "          p = x_window[:, 1].astype(int) # second column of 'protos(protocol index)' [0, 0, 1, 0]\n",
    "\n",
    "          stat_matrix = np.full((N, 3), 1e+7, dtype=np.float32) # Initialize default value to 1e+7\n",
    "\n",
    "          for i in range(N):\n",
    "            t_i = t[p == i] # time sequence of the ith protocol / t : [1.0, 1.2, 1.3, 2.0, ...] / p==i : [True, True, False, True, ...] / t[p==i] : [1.0, 1.2, 2.0] => select protocol by this workflow\n",
    "            if len(t_i) >= 2:\n",
    "                diffs = np.diff(t_i)\n",
    "                mean_val = np.mean(diffs)\n",
    "                stat_matrix[i, 0] = mean_val\n",
    "\n",
    "                if len(diffs) >= 2:\n",
    "                    std_val = np.std(diffs)\n",
    "                    stat_matrix[i, 1] = std_val\n",
    "                if len(diffs) >= 3:\n",
    "                    skew_val = np.abs(skew(diffs))\n",
    "                    stat_matrix[i, 2] = skew_val\n",
    "            \n",
    "          stat_matrix = np.where(stat_matrix == 0, 1e-7, stat_matrix)\n",
    "          stat_matrix = np.log10(stat_matrix)\n",
    "\n",
    "          result.append(stat_matrix)\n",
    "\n",
    "        return np.stack(result) # (num_windows, N, 3)\n",
    "\n",
    "dataset_train = Dataset.towids_train()\n",
    "dataset_test = Dataset.towids_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba0c3ea-b32b-477f-8f71-fdd2e72126cf",
   "metadata": {},
   "source": [
    "### 1. Train file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85121eeb-338c-4acb-a0ce-b1fa9a9c53bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstime</th>\n",
       "      <th>monotime</th>\n",
       "      <th>wirelen</th>\n",
       "      <th>caplen</th>\n",
       "      <th>payload</th>\n",
       "      <th>y</th>\n",
       "      <th>y_desc</th>\n",
       "      <th>ProtocolType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-09-12 09:51:04.715221</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>434</td>\n",
       "      <td>434</td>\n",
       "      <td>[145, 239, 0, 0, 254, 0, 0, 252, 112, 0, 0, 3,...</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>AVTP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-09-12 09:51:04.715245</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>434</td>\n",
       "      <td>434</td>\n",
       "      <td>[145, 239, 0, 0, 254, 0, 0, 252, 112, 0, 0, 3,...</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>AVTP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-09-12 09:51:04.715326</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>434</td>\n",
       "      <td>434</td>\n",
       "      <td>[145, 239, 0, 0, 254, 0, 0, 252, 112, 0, 0, 3,...</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>AVTP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-09-12 09:51:04.715450</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>434</td>\n",
       "      <td>434</td>\n",
       "      <td>[145, 239, 0, 0, 254, 0, 0, 252, 112, 0, 0, 3,...</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>AVTP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-09-12 09:51:04.715559</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>434</td>\n",
       "      <td>434</td>\n",
       "      <td>[145, 239, 0, 0, 254, 0, 0, 252, 112, 0, 0, 3,...</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>AVTP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1203732</th>\n",
       "      <td>2020-09-12 10:00:16.911784</td>\n",
       "      <td>552.196563</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>[220, 166, 50, 94, 72, 71, 220, 166, 50, 93, 2...</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>UDP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1203733</th>\n",
       "      <td>2020-09-12 10:00:16.912231</td>\n",
       "      <td>552.197010</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>[220, 166, 50, 94, 72, 71, 220, 166, 50, 93, 2...</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>UDP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1203734</th>\n",
       "      <td>2020-09-12 10:00:16.912686</td>\n",
       "      <td>552.197465</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>[220, 166, 50, 94, 72, 71, 220, 166, 50, 93, 2...</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>UDP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1203735</th>\n",
       "      <td>2020-09-12 10:00:16.913172</td>\n",
       "      <td>552.197951</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>[220, 166, 50, 94, 72, 71, 220, 166, 50, 93, 2...</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>UDP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1203736</th>\n",
       "      <td>2020-09-12 10:00:16.915384</td>\n",
       "      <td>552.200163</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>[220, 166, 50, 94, 72, 71, 220, 166, 50, 93, 2...</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>UDP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1203334 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           abstime    monotime  wirelen  caplen  \\\n",
       "0       2020-09-12 09:51:04.715221    0.000000      434     434   \n",
       "1       2020-09-12 09:51:04.715245    0.000024      434     434   \n",
       "2       2020-09-12 09:51:04.715326    0.000105      434     434   \n",
       "3       2020-09-12 09:51:04.715450    0.000229      434     434   \n",
       "4       2020-09-12 09:51:04.715559    0.000338      434     434   \n",
       "...                            ...         ...      ...     ...   \n",
       "1203732 2020-09-12 10:00:16.911784  552.196563       60      60   \n",
       "1203733 2020-09-12 10:00:16.912231  552.197010       60      60   \n",
       "1203734 2020-09-12 10:00:16.912686  552.197465       60      60   \n",
       "1203735 2020-09-12 10:00:16.913172  552.197951       60      60   \n",
       "1203736 2020-09-12 10:00:16.915384  552.200163       60      60   \n",
       "\n",
       "                                                   payload  y  y_desc  \\\n",
       "0        [145, 239, 0, 0, 254, 0, 0, 252, 112, 0, 0, 3,...  0  Normal   \n",
       "1        [145, 239, 0, 0, 254, 0, 0, 252, 112, 0, 0, 3,...  0  Normal   \n",
       "2        [145, 239, 0, 0, 254, 0, 0, 252, 112, 0, 0, 3,...  0  Normal   \n",
       "3        [145, 239, 0, 0, 254, 0, 0, 252, 112, 0, 0, 3,...  0  Normal   \n",
       "4        [145, 239, 0, 0, 254, 0, 0, 252, 112, 0, 0, 3,...  0  Normal   \n",
       "...                                                    ... ..     ...   \n",
       "1203732  [220, 166, 50, 94, 72, 71, 220, 166, 50, 93, 2...  0  Normal   \n",
       "1203733  [220, 166, 50, 94, 72, 71, 220, 166, 50, 93, 2...  0  Normal   \n",
       "1203734  [220, 166, 50, 94, 72, 71, 220, 166, 50, 93, 2...  0  Normal   \n",
       "1203735  [220, 166, 50, 94, 72, 71, 220, 166, 50, 93, 2...  0  Normal   \n",
       "1203736  [220, 166, 50, 94, 72, 71, 220, 166, 50, 93, 2...  0  Normal   \n",
       "\n",
       "        ProtocolType  \n",
       "0               AVTP  \n",
       "1               AVTP  \n",
       "2               AVTP  \n",
       "3               AVTP  \n",
       "4               AVTP  \n",
       "...              ...  \n",
       "1203732          UDP  \n",
       "1203733          UDP  \n",
       "1203734          UDP  \n",
       "1203735          UDP  \n",
       "1203736          UDP  \n",
       "\n",
       "[1203334 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProtocolType\n",
      "UDP     846647\n",
      "AVTP    287086\n",
      "PTP      69601\n",
      "Name: count, dtype: int64\n",
      "y_desc\n",
      "Normal    954509\n",
      "C_D        85466\n",
      "P_I        64635\n",
      "F_I        35112\n",
      "M_F        33765\n",
      "C_R        29847\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "display(dataset_train.df)\n",
    "print(dataset_train.df['ProtocolType'].value_counts())\n",
    "print(dataset_train.df['y_desc'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fe8162-9d98-4b7b-8c3d-7da730636cad",
   "metadata": {},
   "source": [
    "### 2. Test file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a725fedd-2f30-4173-879d-e908dd9ad5c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstime</th>\n",
       "      <th>monotime</th>\n",
       "      <th>wirelen</th>\n",
       "      <th>caplen</th>\n",
       "      <th>payload</th>\n",
       "      <th>y</th>\n",
       "      <th>y_desc</th>\n",
       "      <th>ProtocolType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-09-12 10:02:59.795192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>434</td>\n",
       "      <td>434</td>\n",
       "      <td>[145, 239, 0, 0, 254, 0, 0, 252, 112, 0, 0, 3,...</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>AVTP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-09-12 10:02:59.810189</td>\n",
       "      <td>0.014997</td>\n",
       "      <td>434</td>\n",
       "      <td>434</td>\n",
       "      <td>[145, 239, 0, 0, 254, 0, 0, 252, 112, 0, 0, 3,...</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>AVTP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-09-12 10:02:59.810205</td>\n",
       "      <td>0.015013</td>\n",
       "      <td>434</td>\n",
       "      <td>434</td>\n",
       "      <td>[145, 239, 0, 0, 254, 0, 0, 252, 112, 0, 0, 3,...</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>AVTP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-09-12 10:02:59.810295</td>\n",
       "      <td>0.015103</td>\n",
       "      <td>434</td>\n",
       "      <td>434</td>\n",
       "      <td>[145, 239, 0, 0, 254, 0, 0, 252, 112, 0, 0, 3,...</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>AVTP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-09-12 10:02:59.810414</td>\n",
       "      <td>0.015222</td>\n",
       "      <td>434</td>\n",
       "      <td>434</td>\n",
       "      <td>[145, 239, 0, 0, 254, 0, 0, 252, 112, 0, 0, 3,...</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>AVTP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791606</th>\n",
       "      <td>2020-09-12 10:09:36.422031</td>\n",
       "      <td>396.626839</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>[220, 166, 50, 94, 72, 71, 220, 166, 50, 93, 2...</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>UDP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791607</th>\n",
       "      <td>2020-09-12 10:09:36.422535</td>\n",
       "      <td>396.627343</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>[220, 166, 50, 94, 72, 71, 220, 166, 50, 93, 2...</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>UDP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791608</th>\n",
       "      <td>2020-09-12 10:09:36.422997</td>\n",
       "      <td>396.627805</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>[220, 166, 50, 94, 72, 71, 220, 166, 50, 93, 2...</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>UDP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791609</th>\n",
       "      <td>2020-09-12 10:09:36.423462</td>\n",
       "      <td>396.628270</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>[220, 166, 50, 94, 72, 71, 220, 166, 50, 93, 2...</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>UDP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791610</th>\n",
       "      <td>2020-09-12 10:09:36.423838</td>\n",
       "      <td>396.628646</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>[220, 166, 50, 94, 72, 71, 220, 166, 50, 93, 2...</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>UDP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>791324 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          abstime    monotime  wirelen  caplen  \\\n",
       "0      2020-09-12 10:02:59.795192    0.000000      434     434   \n",
       "1      2020-09-12 10:02:59.810189    0.014997      434     434   \n",
       "2      2020-09-12 10:02:59.810205    0.015013      434     434   \n",
       "3      2020-09-12 10:02:59.810295    0.015103      434     434   \n",
       "4      2020-09-12 10:02:59.810414    0.015222      434     434   \n",
       "...                           ...         ...      ...     ...   \n",
       "791606 2020-09-12 10:09:36.422031  396.626839       60      60   \n",
       "791607 2020-09-12 10:09:36.422535  396.627343       60      60   \n",
       "791608 2020-09-12 10:09:36.422997  396.627805       60      60   \n",
       "791609 2020-09-12 10:09:36.423462  396.628270       60      60   \n",
       "791610 2020-09-12 10:09:36.423838  396.628646       60      60   \n",
       "\n",
       "                                                  payload  y  y_desc  \\\n",
       "0       [145, 239, 0, 0, 254, 0, 0, 252, 112, 0, 0, 3,...  0  Normal   \n",
       "1       [145, 239, 0, 0, 254, 0, 0, 252, 112, 0, 0, 3,...  0  Normal   \n",
       "2       [145, 239, 0, 0, 254, 0, 0, 252, 112, 0, 0, 3,...  0  Normal   \n",
       "3       [145, 239, 0, 0, 254, 0, 0, 252, 112, 0, 0, 3,...  0  Normal   \n",
       "4       [145, 239, 0, 0, 254, 0, 0, 252, 112, 0, 0, 3,...  0  Normal   \n",
       "...                                                   ... ..     ...   \n",
       "791606  [220, 166, 50, 94, 72, 71, 220, 166, 50, 93, 2...  0  Normal   \n",
       "791607  [220, 166, 50, 94, 72, 71, 220, 166, 50, 93, 2...  0  Normal   \n",
       "791608  [220, 166, 50, 94, 72, 71, 220, 166, 50, 93, 2...  0  Normal   \n",
       "791609  [220, 166, 50, 94, 72, 71, 220, 166, 50, 93, 2...  0  Normal   \n",
       "791610  [220, 166, 50, 94, 72, 71, 220, 166, 50, 93, 2...  0  Normal   \n",
       "\n",
       "       ProtocolType  \n",
       "0              AVTP  \n",
       "1              AVTP  \n",
       "2              AVTP  \n",
       "3              AVTP  \n",
       "4              AVTP  \n",
       "...             ...  \n",
       "791606          UDP  \n",
       "791607          UDP  \n",
       "791608          UDP  \n",
       "791609          UDP  \n",
       "791610          UDP  \n",
       "\n",
       "[791324 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProtocolType\n",
      "UDP     563731\n",
      "AVTP    198013\n",
      "PTP      29580\n",
      "Name: count, dtype: int64\n",
      "y_desc\n",
      "Normal    660490\n",
      "C_D        41203\n",
      "C_R        29847\n",
      "P_I        26013\n",
      "F_I        16962\n",
      "M_F        16809\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "display(dataset_test.df)\n",
    "print(dataset_test.df['ProtocolType'].value_counts())\n",
    "print(dataset_test.df['y_desc'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff753878-a533-4a22-b13d-b462f423a464",
   "metadata": {},
   "source": [
    "- Create train/validation/test sets by dividing the two packet dump datasets (dataset_train, dataset_test) into different time ranges.\n",
    "- Organize the number of malicious traffic (intrusion) and normal traffic (benign) in these sets into a table as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61327a86-842a-463e-a1db-f4ce6a769f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>y</th>\n",
       "      <th>Purpose</th>\n",
       "      <th>Time range</th>\n",
       "      <th>Benign</th>\n",
       "      <th>Intrusion</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Packet dump</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Packet dump 1</th>\n",
       "      <td>Train</td>\n",
       "      <td>[5.00, 60.00]</td>\n",
       "      <td>97,715</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Packet dump 1</th>\n",
       "      <td>Validation</td>\n",
       "      <td>[60.00, 71.11]</td>\n",
       "      <td>19,606</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Packet dump 1</th>\n",
       "      <td>Test</td>\n",
       "      <td>[71.11, 547.20]</td>\n",
       "      <td>819,586</td>\n",
       "      <td>248,080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Packet dump 2</th>\n",
       "      <td>Train</td>\n",
       "      <td>[5.00, 80.00]</td>\n",
       "      <td>130,520</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Packet dump 2</th>\n",
       "      <td>Validation</td>\n",
       "      <td>[80.00, 91.88]</td>\n",
       "      <td>19,943</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Packet dump 2</th>\n",
       "      <td>Test</td>\n",
       "      <td>[91.89, 391.63]</td>\n",
       "      <td>496,151</td>\n",
       "      <td>129,226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "y                 Purpose       Time range   Benign Intrusion\n",
       "Packet dump                                                  \n",
       "Packet dump 1       Train    [5.00, 60.00]   97,715         0\n",
       "Packet dump 1  Validation   [60.00, 71.11]   19,606         0\n",
       "Packet dump 1        Test  [71.11, 547.20]  819,586   248,080\n",
       "Packet dump 2       Train    [5.00, 80.00]  130,520         0\n",
       "Packet dump 2  Validation   [80.00, 91.88]   19,943         0\n",
       "Packet dump 2        Test  [91.89, 391.63]  496,151   129,226"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Arguments to be passed to the do function: \n",
    "# [dataset, purpose, start time, end time, whether to remove the last 5 seconds of noise]\n",
    "args = [\n",
    "    [dataset_train, 'Train', 5, 60, False],\n",
    "    [dataset_train, 'Validation', 60, 71.11, False],\n",
    "    [dataset_train, 'Test', 71.11, None, True],\n",
    "    [dataset_test, 'Train', 5, 80, False],\n",
    "    [dataset_test, 'Validation', 80, 91.88, False],\n",
    "    [dataset_test, 'Test', 91.89, None, True],\n",
    "]\n",
    "\n",
    "def do(dataset, purpose, time_start, time_end, trim_last_5sec):\n",
    "    name = 'Packet dump 1' if dataset is dataset_train else 'Packet dump 2'\n",
    "\n",
    "    dataset = dataset.trim(time_start, time_end, is_absolute=True) # slice [time_start, time_end] part only in the entire dataset\n",
    "    if trim_last_5sec: # Remove noise remaining after the data collection step\n",
    "        dataset = dataset.trim(time_end=5, is_absolute=False)\n",
    "        time_end = dataset.df['monotime'].max() # Since 'time_end' has changed after removing noise, update it again with the actual maximum time\n",
    "    a = dataset.df['y'].value_counts() \n",
    "    a.name = name \n",
    "    a['Purpose'] = purpose \n",
    "    a['Time range'] = '[{:.2f}, {:.2f}]'.format(time_start, time_end)\n",
    "    a = a.rename({0: 'Benign', 1: 'Intrusion'}) # 0 as benign, 1 as intrusion\n",
    "    a = a.reindex(['Purpose', 'Time range', 'Benign', 'Intrusion'], fill_value=0)\n",
    "    return a, dataset\n",
    "\n",
    "\n",
    "list_output = list()\n",
    "list_dataset_sub = list() ######### From here, you can retrieve the Dataset instance as needed.\n",
    "for arg in args:\n",
    "    output, dataset_sub = do(*arg)\n",
    "    list_output.append(output)\n",
    "    list_dataset_sub.append(dataset_sub)\n",
    "\n",
    "df = pd.DataFrame(list_output)\n",
    "df.index.name = 'Packet dump'\n",
    "df[['Benign', 'Intrusion']] = df[['Benign', 'Intrusion']].map('{:,}'.format)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0d70b4-0c82-4082-9436-216ec3dd8fa7",
   "metadata": {},
   "source": [
    "### TimeseriesGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a259981-78a0-4feb-a167-b0a761f79363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class TimeseriesGenerator:\n",
    "    def __init__(self, data, length, sampling_rate=1, stride=1,\n",
    "                 start_index=0, end_index=None,\n",
    "                 shuffle=False, reverse=False, batch_size=128, label=None):\n",
    "        self.data = data\n",
    "        self.length = length\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.stride = stride\n",
    "        self.start_index = start_index + length\n",
    "        if end_index is None:\n",
    "            end_index = len(data)\n",
    "        self.end_index = end_index\n",
    "        self.shuffle = shuffle\n",
    "        self.reverse = reverse\n",
    "        self.batch_size = batch_size\n",
    "        self.label = label if label is None else np.array(label)\n",
    "        if self.start_index > self.end_index:\n",
    "            raise ValueError(\n",
    "                \"`start_index+length=%i > end_index=%i` \"\n",
    "                \"is disallowed, as no part of the sequence \"\n",
    "                \"would be left to be used as current step.\"\n",
    "                % (self.start_index, self.end_index)\n",
    "            )\n",
    "\n",
    "    def __len__(self):\n",
    "        return (self.end_index - self.start_index + self.batch_size * self.stride) // (self.batch_size * self.stride)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        rows = self.__index_to_row__(index)\n",
    "        samples, y = self.__compile_batch__(rows)\n",
    "        return samples, y\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i in range(len(self)):\n",
    "            yield self[i]\n",
    "    \n",
    "    def __index_to_row__(self, index):  # Returns a list of rows that will compose a given batch (index). len(rows) is equal to the batch size.\n",
    "        if self.shuffle:\n",
    "            rows = np.random.randint(self.start_index, self.end_index + 1, size=self.batch_size)\n",
    "        else:\n",
    "            i = self.start_index + self.batch_size * self.stride * index\n",
    "            rows = np.arange(i, min(i + self.batch_size * self.stride, self.end_index + 1), self.stride)\n",
    "        return rows\n",
    "\n",
    "    def __compile_batch__(self, rows):  # Generate time series features for each given row.\n",
    "        samples = np.array([self.data[row - self.length: row: self.sampling_rate] for row in rows])\n",
    "        if self.reverse:\n",
    "            samples = samples[:, ::-1, ...]\n",
    "        if self.length == 1:\n",
    "            samples = np.squeeze(samples)\n",
    "\n",
    "        if self.label is None:\n",
    "            return samples, samples\n",
    "        else:\n",
    "            return samples, self.label[rows - self.length]\n",
    "\n",
    "    @property\n",
    "    def output_shape(self):\n",
    "        x, y = self[0]\n",
    "        return x.shape, y.shape\n",
    "\n",
    "    @property\n",
    "    def num_samples(self):\n",
    "        count = 0\n",
    "        for x, y in self:\n",
    "            count += x.shape[0]\n",
    "        return count\n",
    "\n",
    "    def __str__(self):\n",
    "        return '<TimeseriesGenerator data.shape={} / num_batches={:,} / output_shape={}>'.format(\n",
    "            self.data.shape, len(self), self.output_shape,\n",
    "        )\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3381419c-28bd-45c4-bc75-dbb30e1adc41",
   "metadata": {},
   "source": [
    "### Define new Dataset Class \n",
    "- Converting the shape of each FG1-3, to match the dimension as an input value of the Autoencoder model afterwards\n",
    "    - x = (T, P, S)\n",
    "\n",
    "- dataset[i][0] → ((9,), (2048, 9), (9,))\n",
    "- dataset[i][1] → ((9,), (2048, 9), (9,)) ⇒ x == y since it's an autoencoder model\n",
    "- dataloader[i][0] → ((b, 9), (b, 2048, 9), (b, 9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fc85be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms # 데이터 전처리(변환) 도구"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "195c7a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0 is avaiable\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"{device} is avaiable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0563769-13e0-465e-9e10-408abb71b554",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AEGenerator(Dataset):\n",
    "    def __init__(self, T, P, S): # 데이터셋 초기화, 파일 불러오기, 전처리 정의\n",
    "        '''\n",
    "        T : nparray (n, 3, 3)\n",
    "        P : nparray (n, 9)\n",
    "        S : nparray (n, 3, 3)\n",
    "        '''\n",
    "        self.T = T\n",
    "        self.P = P\n",
    "        self.S = S\n",
    "        self.n = T.shape[0]\n",
    "        pass\n",
    "    \n",
    "    def __len__(self): # 데이터셋의 총 샘플 수 리턴\n",
    "        return self.n\n",
    "\n",
    "    def __getitem__(self, idx): # 하나의 샘플(x, y)를 리턴\n",
    "        '''\n",
    "        T : (3, 3) -> (9,)\n",
    "        self.T[idx] : nparray(3, 3)\n",
    "        from_numpy() : nparray -> torch tensor\n",
    "        .flatten() : 2D (3, 3) -> 1D (9, )\n",
    "        \n",
    "        ~~Intuitive Example~~\n",
    "        self.T[idx] = [[1, 2, 3],\n",
    "                    [4, 5, 6],\n",
    "                    [7, 8, 9]]\n",
    "        Output : tensor([1., 2., 3., 4., 5., 6., 7., 8., 9.])\n",
    "        '''\n",
    "        t = torch.from_numpy(self.T[idx].astype('float32')).flatten()\n",
    "        \n",
    "        '''\n",
    "        P : (9,) -> (2048, 9)\n",
    "        self.P[max(0, idx-2047):idx+1] : 과거 2048개의 row (없으면 시작부터 count)\n",
    "\n",
    "        e.g. if idx=5, self.P[0:6] -> shape(6, 9)\n",
    "\n",
    "        padding 작업 : 과거 row 수 < 2048(window size)이면 0으로 채운 tensor를 만들어서 앞에 붙이고, .cat()으로 두 tensor를 concat\n",
    "\n",
    "        e.g.\n",
    "        p.shape = (6, 9)\n",
    "        padding.shape = (2042, 9) # 2048 - 6 = 2042로 계산\n",
    "        p.shape = (2048, 9) # torch.cat\n",
    "        '''\n",
    "        p = torch.from_numpy(self.P[max(0, idx-2047):idx+1].astype('float32'))\n",
    "        if p.shape[0] < 2048:\n",
    "            padding = torch.zeros((2048 - p.shape[0], 9), dtype=torch.float32)\n",
    "            p = torch.cat([padding, p], dim = 0)\n",
    "\n",
    "\n",
    "        '''\n",
    "        T와 동일하게 S : (3, 3) -> (9,)\n",
    "        self.S[idx] = [[0, 1, 0],\n",
    "                    [1, 0, 1],\n",
    "                    [0, 1, 0]]\n",
    "        => tensor([0., 1., 0., 1., 0., 1., 0., 1., 0.])\n",
    "        '''\n",
    "        s = torch.from_numpy(self.S[idx].astype('float32')).flatten()\n",
    "\n",
    "        x = y = (t, p, s)\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0898728f-62ed-4630-b427-8e1a87fedc77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (97715,)\n",
      "Window size: 2048\n",
      "Generator length: 95668\n",
      "FG1 original shape: (95668, 3, 3)\n",
      "\n",
      "FG2 original shape: (97715, 9)\n",
      "\n",
      "Data shape: (97715, 2)\n",
      "Window size: 2048\n",
      "FG3 original shape: (95668, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "original_dataset = list_dataset_sub[0] # 'train' part in dataset_train\n",
    "T = original_dataset.do_fg1_transition_matrix()\n",
    "print(\"FG1 original shape:\", T.shape)\n",
    "print()\n",
    "P = original_dataset.do_fg2_payload()\n",
    "print(\"FG2 original shape:\", P.shape)\n",
    "print()\n",
    "S = original_dataset.do_fg3_statistics()\n",
    "print(\"FG3 original shape:\", S.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aab019e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AEGenerator(T, P, S)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True) # n = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9f6255d-f844-4029-b3fc-d51e60a3910a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T shape: torch.Size([32, 9])\n",
      "P shape: torch.Size([32, 2048, 9])\n",
      "S shape: torch.Size([32, 9])\n"
     ]
    }
   ],
   "source": [
    "for batch_x, _ in dataloader:\n",
    "    t_batch, p_batch, s_batch = batch_x\n",
    "    print(f'T shape: {t_batch.shape}') # (32, 9)\n",
    "    print(f'P shape: {p_batch.shape}') # (32, 2048, 9)\n",
    "    print(f'S shape: {s_batch.shape}') # (32, 9)\n",
    "    assert t_batch.shape[0] == p_batch.shape[0] == s_batch.shape[0], \"T, P, S must have same number of samples!\"\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f08afb6",
   "metadata": {},
   "source": [
    "- So as above, currently the shape of the elements in input tuple x is as below:\n",
    "    - t : (b, 9)\n",
    "    - p : (b, 2048, 9)\n",
    "    - s : (b, 9)\n",
    "- Encoder Architecture in the paper:\n",
    "    - T : Flatten -> Dense(64) -> Dense(64)\n",
    "    - P : Seperable Conv1D -> ... (n layers: depends on w) -> Seperable Conv1D -> Seperable Conv1D(576)\n",
    "    - S : Flatten -> Dense(64) -> Dense(64)\n",
    "    - latent vector h : 704 = 64 + 64 + 576\n",
    "- Decoder Architecture in the paper:\n",
    "    - latent space h(704) has 3 independent network : T', P', S'\n",
    "    - T', S' : Dense(64) -> Dense(64) -> Dense(9)\n",
    "    - P' : Transposed SeperableConv1D n layers (last output's channel=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd11bd0",
   "metadata": {},
   "source": [
    "- Pytorch doesn't directly afford SeperableConv1d...\n",
    "    - let's implement in w/ depthwise + pointwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cadc87c-cd54-4a6a-97f3-c7466a8d7e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeperableConv1d(nn.Module):\n",
    "    def __init__(self, input_c, output_c, kernel_size, **kwargs):\n",
    "        super().__init__()\n",
    "        self.depthwise = nn.Conv1d(input_c, input_c, kernel_size, groups=input_c, **kwargs)\n",
    "        self.pointwise = nn.Conv1d(input_c, output_c, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ddf6837",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeparableConvTranspose1d(nn.Module):\n",
    "    def __init__(self, input_c, output_c, kernel_size, **kwargs):\n",
    "        super().__init__()\n",
    "        self.depthwise = nn.ConvTranspose1d(input_c, input_c, kernel_size, groups=input_c, **kwargs)\n",
    "        self.pointwise = nn.ConvTranspose1d(input_c, output_c, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2585e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, num_p_layers=8):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        # 1. Encoder definition\n",
    "        self.t_fc1 = nn.Linear(9, 64)\n",
    "        self.t_fc2 = nn.Linear(64, 64)\n",
    "\n",
    "        self.s_fc1 = nn.Linear(9, 64)\n",
    "        self.s_fc2 = nn.Linear(64, 64)\n",
    "\n",
    "        p_layers = []\n",
    "        in_ch = 9\n",
    "        for i in range(num_p_layers):\n",
    "            out_ch = 64 if i < num_p_layers - 1 else 576\n",
    "            p_layers.append(SeperableConv1d(in_ch, out_ch, kernel_size=3, padding=1))\n",
    "            in_ch = out_ch\n",
    "        self.p_layers = nn.ModuleList(p_layers)\n",
    "\n",
    "        \n",
    "        # 2. Decoder definition\n",
    "        self.t_dec_fc1 = nn.Linear(704, 64)\n",
    "        self.t_dec_fc2 = nn.Linear(64, 64)\n",
    "        self.t_dec_fc3 = nn.Linear(64, 9)\n",
    "\n",
    "        self.s_dec_fc1 = nn.Linear(704, 64)\n",
    "        self.s_dec_fc2 = nn.Linear(64, 64)\n",
    "        self.s_dec_fc3 = nn.Linear(64, 9)\n",
    "\n",
    "        p_dec_layers = []\n",
    "        in_ch = 704\n",
    "        for i in range(num_p_layers):\n",
    "            out_ch = 64 if i < num_p_layers - 1 else 9\n",
    "            p_dec_layers.append(SeparableConvTranspose1d(in_ch, out_ch, kernel_size=3, padding=1))\n",
    "            in_ch = out_ch\n",
    "        self.p_dec_layers = nn.ModuleList(p_dec_layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        t, p, s = x\n",
    "\n",
    "        # 1. Encoder\n",
    "        t = F.relu(self.t_fc1(t))\n",
    "        t = F.relu(self.t_fc2(t))\n",
    "\n",
    "        s = F.relu(self.s_fc1(s))\n",
    "        s = F.relu(self.s_fc2(s))\n",
    "\n",
    "        p = p.permute(0, 2, 1) # (b, 9, 2048)\n",
    "        for layer in self.p_layers[:-1]:\n",
    "            p = F.relu(layer(p))\n",
    "        p = self.p_layers[-1](p)\n",
    "        p = p.mean(dim=2) # (b, 576)\n",
    "\n",
    "\n",
    "        # latent vector\n",
    "        h = torch.cat([t, p, s], dim=1)\n",
    "\n",
    "\n",
    "        # 2. Decoder\n",
    "        t = F.relu(self.t_dec_fc1(h))\n",
    "        t = F.relu(self.t_dec_fc2(t))\n",
    "        t = self.t_dec_fc3(t)\n",
    "\n",
    "        s = F.relu(self.s_dec_fc1(h))\n",
    "        s = F.relu(self.s_dec_fc2(s))\n",
    "        s = self.s_dec_fc3(s)\n",
    "\n",
    "        p = h. unsqueeze(2).repeat(1, 1, 2048) # (b, 704, 2048)\n",
    "        for layer in self.p_dec_layers[:-1]:\n",
    "            p = F.relu(layer(p))\n",
    "        p = self.p_dec_layers[-1](p) # (b, 9, 2048)\n",
    "        p = p.permute(0, 2, 1) # (b, 2048, 9)\n",
    "\n",
    "        return t, p, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "129c59ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder().to(device)\n",
    "# loss function\n",
    "criterion = nn.MSELoss() \n",
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f40b4476-9756-426e-bdbf-aea911dbaec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just a simple test~\n",
    "model = Autoencoder(num_p_layers=8)\n",
    "t = torch.randn(8, 9)\n",
    "p = torch.randn(8, 2048, 9)\n",
    "s = torch.randn(8, 9)\n",
    "\n",
    "h = model((t, p, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7250609d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (32) must match the size of tensor b (2048) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m optimizer.zero_grad()       \u001b[38;5;66;03m# 이전 배치에서 계산된 기울기 초기화\u001b[39;00m\n\u001b[32m      9\u001b[39m t_hat, p_hat, s_hat = model(x)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m loss = (\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_hat\u001b[49m\u001b[43m)\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_hat\u001b[49m\u001b[43m)\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[32;43m2\u001b[39;49m + (s - s_hat)**\u001b[32m2\u001b[39m).mean() \u001b[38;5;66;03m# MSE\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m'''\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[33;03m# T loss\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[33;03mloss_t = F.mse_loss(t_hat, t, reduction='mean')  # PyTorch의 mean은 전체 평균\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m \u001b[33;03mloss = loss_t + loss_p + loss_s        \u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[33;03m'''\u001b[39;00m\n\u001b[32m     24\u001b[39m loss.backward()             \u001b[38;5;66;03m# 역전파를 통해 각 파라미터에 대한 기울기를 계산\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (32) must match the size of tensor b (2048) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "epoch1 = 2 # 논문에서는 20이지만 일단 작은 수로 잘 돌아가는지 테스트\n",
    "for epoch in range(epoch1):\n",
    "    loss_AE = 0.0              # loss값 누적할 변수 초기화\n",
    "    for x, _ in dataloader:\n",
    "        t, p, s = (item.to(device) for item in x) # 배치의 입력 데이터를 gpu로 보냄\n",
    "        t, p, s = x\n",
    "\n",
    "        optimizer.zero_grad()       # 이전 배치에서 계산된 기울기 초기화\n",
    "        t_hat, p_hat, s_hat = model(x)\n",
    "        loss = ((t - t_hat)**2 + (p - p_hat)**2 + (s - s_hat)**2).mean() # MSE\n",
    "\n",
    "        '''\n",
    "        # T loss\n",
    "        loss_t = F.mse_loss(t_hat, t, reduction='mean')  # PyTorch의 mean은 전체 평균\n",
    "        # P loss\n",
    "        loss_p = F.mse_loss(p_hat, p, reduction='mean')\n",
    "        # S loss\n",
    "        loss_s = F.mse_loss(s_hat, s, reduction='mean')\n",
    "\n",
    "        # loss 합\n",
    "        loss = loss_t + loss_p + loss_s        \n",
    "        '''\n",
    "\n",
    "        loss.backward()             # 역전파를 통해 각 파라미터에 대한 기울기를 계산\n",
    "        optimizer.step()            # weight update\n",
    "        loss_AE += loss.item() # batch의 loss값을 누적해 epoch 전체 loss를 계산\n",
    "    cost = loss_AE / len(dataloader) # average loss 값 계산\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        print(f\"[{epoch + 1}] loss: {cost:.3f}\") # 현재 epoch와 평균 손실 값을 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e456fc27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
